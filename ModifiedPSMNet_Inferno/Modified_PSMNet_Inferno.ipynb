{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gt_relupsm_clache_proj_final_inferno.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVL8DpTH3ABW"
      },
      "source": [
        "**Import data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c-Q5-YlGueV"
      },
      "source": [
        "# Download data specific files\n",
        "!gdown https://drive.google.com/uc?id=1JxhXXqcj0TdVkDAQiLhFWh3TpaPrNMG3\n",
        "!unzip cats_expt.zip\n",
        "!mkdir models"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq-JkjviC-_c",
        "outputId": "7a16b977-c713-40cb-d989-211a329e621f"
      },
      "source": [
        "# Mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BbqdfAY3Kop"
      },
      "source": [
        "**Import necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJhzdR1HG-QZ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv3unntQ3Of0"
      },
      "source": [
        "**Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L8ZbRGWHbEn"
      },
      "source": [
        "class StereoDataset(Dataset):\n",
        "  def __init__(self, data_dir,\n",
        "               device,\n",
        "               dataset_name = 'CATS',\n",
        "               mode = 'train',\n",
        "               transform=None\n",
        "               ):\n",
        "    super(StereoDataset, self).__init__()\n",
        "    self.data_dir     = data_dir\n",
        "    self.dataset_name = dataset_name\n",
        "    self.mode         = mode\n",
        "    self.transform    = transform\n",
        "    self.device       = device\n",
        "    # Pre-processing\n",
        "    self.clahe        = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "\n",
        "\n",
        "    cats_data_dict = {\n",
        "        'train': 'cats_final/filenames1/cats_train.txt',\n",
        "        'val'  : 'cats_final/filenames1/cats_val.txt',\n",
        "        'test' : 'cats_final/filenames1/cats_test.txt' \n",
        "    }\n",
        "\n",
        "    cmu_data_dict = {\n",
        "        'train': 'filenames1/cmu_train.txt',\n",
        "        'val'  : 'filenames1/cmu_val.txt',\n",
        "        'test' : 'filenames1/cmu_test.txt' \n",
        "    }\n",
        "\n",
        "    dataset_name_dict = {\n",
        "        'CATS': cats_data_dict,\n",
        "        'CMU': cmu_data_dict,\n",
        "    }\n",
        "\n",
        "    assert dataset_name in dataset_name_dict.keys()\n",
        "\n",
        "    self.left_imgs, self.right_imgs, self.gt_imgs = self.get_file_names(dataset_name_dict[dataset_name][mode])\n",
        "\n",
        "  def get_file_names(self, file_name_list):\n",
        "    f = open(file_name_list, 'r')\n",
        "    f = f.readlines()       \n",
        "    left_arr, right_arr, gt_arr = [], [], []\n",
        "\n",
        "    for line in f:\n",
        "      line      = line.split('\\n')[0]\n",
        "      left_img  = self.data_dir + 'left/'  + line\n",
        "      right_img = self.data_dir + 'right/' + line\n",
        "      gt_img    = self.data_dir + 'disp/'  + line.split('.')[0] + '.txt'\n",
        "\n",
        "      gt_arr.append(gt_img)\n",
        "      left_arr.append(left_img)\n",
        "      right_arr.append(right_img)\n",
        "\n",
        "    return left_arr, right_arr, gt_arr\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    left_img_path  = self.left_imgs[index]\n",
        "    right_img_path = self.right_imgs[index]\n",
        "    gt_img_path    = self.gt_imgs[index]\n",
        "\n",
        "    # Read the images\n",
        "    left_img  = cv2.imread(left_img_path,  0)\n",
        "    right_img = cv2.imread(right_img_path, 0)\n",
        "    \n",
        "    # Apply pre-processing\n",
        "    left_img   = self.clahe.apply(left_img)\n",
        "    right_img  = self.clahe.apply(right_img)\n",
        "\n",
        "    # Perform clipping\n",
        "    disp_img  = np.genfromtxt(gt_img_path, delimiter=',')\n",
        "    disp_img  = (disp_img.astype(float))\n",
        "    disp_img  += 70\n",
        "    disp_img[disp_img < 0]   = 0.0\n",
        "    disp_img[disp_img > 192] = 192.0\n",
        "    \n",
        "    left_img  = torch.unsqueeze(torch.from_numpy(left_img.astype(float)).float(),dim=0)   #permute(2, 0, 1)\n",
        "    right_img = torch.unsqueeze(torch.from_numpy(right_img.astype(float)).float(),dim=0)  #permute(2, 0, 1)\n",
        "    disp_img  = (torch.from_numpy(disp_img).float())\n",
        "\n",
        "    if self.transform is not None:\n",
        "      left_img, right_img, disp_img = self.transform(left_img), self.transform(right_img), self.transform(disp_img)\n",
        "    \n",
        "    return left_img, right_img, disp_img\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.left_imgs)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4vaiOkF4Fyk"
      },
      "source": [
        "**Model architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF_SIdWoHdZw"
      },
      "source": [
        "# Basic residual block from Kaimeng He's Resnet Paper:\n",
        "\n",
        "class residual_block(nn.Module):\n",
        "  def __init__(self,in_channel,out_channel,kernel,stride,padding,dilation):\n",
        "    super(residual_block, self).__init__()\n",
        "\n",
        "    self.in_channels = in_channel\n",
        "    self.out_channels = out_channel\n",
        "    self.stride = stride\n",
        "    self.dilation = dilation\n",
        "\n",
        "    layers = [nn.Conv2d(in_channel, out_channel, kernel, stride, padding, dilation = dilation, bias = False), nn.BatchNorm2d(out_channel),nn.ReLU(inplace=True),\n",
        "              nn.Conv2d(out_channel, out_channel, kernel, 1, padding,dilation=dilation, bias = False), nn.BatchNorm2d(out_channel)]\n",
        "\n",
        "    self.layers = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    if self.in_channels == self.out_channels: # if-else block deals with the condition when number of channels change across residual/skip connection\n",
        "      input = x\n",
        "    else:\n",
        "      diff = self.out_channels - self.in_channels\n",
        "      x1 = torch.zeros(x.shape[0],diff,x.shape[2],x.shape[3]).to(device)\n",
        "      input = torch.cat((x,x1),dim=1)\n",
        "\n",
        "\n",
        "    if self.stride == 2: # Downsample input tensor when number of channels change across residual connections and stride is 2\n",
        "      input = F.max_pool2d(input,kernel_size = 2,stride = 2)\n",
        "\n",
        "    output = self.layers(x)\n",
        "\n",
        "    output += input # PSM-Net paper does not use ReLU after skip connection\n",
        "    return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFxPrgoRHfF7"
      },
      "source": [
        "# CNN module of PSMNet:\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN,self).__init__()\n",
        "\n",
        "    layers_1 = [\n",
        "              nn.Conv2d(in_channels = 1,out_channels = 32, kernel_size = 3, stride = 2, padding = 1, bias = False), nn.BatchNorm2d(32),nn.ReLU(inplace=True),   # H/2 x W/2 x 32 Conv0_1\n",
        "              nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(32),nn.ReLU(inplace=True), # H/2 x W/2 x 32 Conv0_2\n",
        "              nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(32),nn.ReLU(inplace=True), # H/2 x W/2 x 32 Conv0_3\n",
        "              residual_block(32,32,3,1,1,1), residual_block(32,32,3,1,1,1), residual_block(32,32,3,1,1,1), # H/2 x W/2 x 32 Conv1_ 1-3\n",
        "              residual_block(32,64,3,2,1,1), residual_block(64,64,3,1,1,1), residual_block(64,64,3,1,1,1), residual_block(64,64,3,1,1,1), # Conv2_ 1-4\n",
        "              residual_block(64,64,3,1,1,1), residual_block(64,64,3,1,1,1), residual_block(64,64,3,1,1,1), residual_block(64,64,3,1,1,1), # Conv2_ 5-8\n",
        "              residual_block(64,64,3,1,1,1), residual_block(64,64,3,1,1,1), residual_block(64,64,3,1,1,1), residual_block(64,64,3,1,1,1), # Conv2_ 9-12\n",
        "              residual_block(64,64,3,1,1,1), residual_block(64,64,3,1,1,1), residual_block(64,64,3,1,1,1), residual_block(64,64,3,1,1,1) # H/4 x W/4 x 64 Conv2_ 13-16\n",
        "              ]\n",
        "              \n",
        "    layers_2 = [      \n",
        "              residual_block(64,128,3,1,2,2), residual_block(128,128,3,1,2,2), residual_block(128,128,3,1,2,2), # H/4 x W/4 x 64 Conv3_ 1-3\n",
        "              residual_block(128,128,3,1,4,4), residual_block(128,128,3,1,4,4), residual_block(128,128,3,1,4,4) # H/4 x W/4 x 64 Conv4_ 1-3 \n",
        "              ]\n",
        "\n",
        "    # Split sequential block into two lists to extract intermediate outputs for concatentaion in subsequent modules:\n",
        "\n",
        "    self.layers_1 = nn.Sequential(*layers_1)\n",
        "    self.layers_2 = nn.Sequential(*layers_2)\n",
        "\n",
        "  def forward(self,x):\n",
        "    cnn_concat1 = self.layers_1(x)\n",
        "    cnn_concat2 = self.layers_2(cnn_concat1)\n",
        "    return cnn_concat1,cnn_concat2 # Return two outputs(used further in concat operation)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbwBAr29HgrU"
      },
      "source": [
        "# SPP module of PSMNet:\n",
        "\n",
        "class SPP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SPP,self).__init__()\n",
        "\n",
        "    self.cnn_module = CNN()\n",
        "\n",
        "    self.branch1 = nn.Sequential(\n",
        "                  nn.AvgPool2d(16,stride=16),\n",
        "                  nn.Conv2d(in_channels = 128,out_channels = 32, kernel_size = 1, stride = 1, padding = 0, dilation = 1, bias = False), nn.BatchNorm2d(32), nn.ReLU(inplace=True)\n",
        "                  ) # branch_1\n",
        "\n",
        "    self.branch2 = nn.Sequential(\n",
        "                  nn.AvgPool2d(8,stride=8),\n",
        "                  nn.Conv2d(in_channels = 128,out_channels = 32, kernel_size = 1, stride = 1, padding = 0, dilation = 1, bias = False), nn.BatchNorm2d(32), nn.ReLU(inplace=True)\n",
        "                  ) # branch_2\n",
        "\n",
        "    self.branch3 = nn.Sequential(\n",
        "                  nn.AvgPool2d(4,stride=4),\n",
        "                  nn.Conv2d(in_channels = 128,out_channels = 32, kernel_size = 1, stride = 1, padding = 0, dilation = 1, bias = False), nn.BatchNorm2d(32), nn.ReLU(inplace=True)\n",
        "                  ) # branch_3\n",
        "\n",
        "    self.branch4 = nn.Sequential(\n",
        "                  nn.AvgPool2d(2,stride=2),\n",
        "                  nn.Conv2d(in_channels = 128,out_channels = 32, kernel_size = 1, stride = 1, padding = 0, bias = False), nn.BatchNorm2d(32), nn.ReLU(inplace=True)\n",
        "                  ) # branch_4\n",
        "\n",
        "    self.final_conv = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels = 320, out_channels =128, kernel_size = 3, stride = 1, padding = 1, bias = False), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(in_channels = 128, out_channels = 32, kernel_size = 1, stride = 1, bias = False)\n",
        "                    ) # fusion\n",
        "\n",
        "  def forward(self,x):\n",
        "    conv2_16,conv4_3 = self.cnn_module(x)\n",
        "    branch1 = self.branch1(conv4_3)\n",
        "    branch2 = self.branch2(conv4_3)\n",
        "    branch3 = self.branch3(conv4_3)\n",
        "    branch4 = self.branch4(conv4_3)\n",
        "    branch1 = F.interpolate(branch1, (conv4_3.shape[2],conv4_3.shape[3]),mode='bilinear')\n",
        "    branch2 = F.interpolate(branch2, (conv4_3.shape[2],conv4_3.shape[3]),mode='bilinear')\n",
        "    branch3 = F.interpolate(branch3, (conv4_3.shape[2],conv4_3.shape[3]),mode='bilinear')\n",
        "    branch4 = F.interpolate(branch4, (conv4_3.shape[2],conv4_3.shape[3]),mode='bilinear')\n",
        "    concat_tensor = torch.cat((conv2_16,conv4_3,branch1,branch2,branch3,branch4), dim = 1) # Concat all pooled components (Concat block of SPP)\n",
        "    spp_output = self.final_conv(concat_tensor)\n",
        "    return spp_output\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS4Nl9wEHiTv"
      },
      "source": [
        "# Custom class for disparity regression (Adopted from PSM-Net GitHub repo):\n",
        "\n",
        "class disp_reg(nn.Module):\n",
        "  def __init__(self, maxdisp):\n",
        "    super(disp_reg, self).__init__()\n",
        "    self.disp = Variable(torch.Tensor(np.reshape(np.array(range(maxdisp)),[1,maxdisp,1,1])).cuda(), requires_grad=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    disp = self.disp.repeat(x.size()[0],1,x.size()[2],x.size()[3])\n",
        "    out  = torch.sum(x*disp,1)\n",
        "    return out\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jx0x99-Hkki"
      },
      "source": [
        "# PSM-Net model class definition (Trying with simple Basic 3D CNN architecture first!):\n",
        "# Should take a stereo pair as input and do the rest. \n",
        "\n",
        "class PSMNET(nn.Module):\n",
        "  def __init__(self,max_disp):\n",
        "    super(PSMNET,self).__init__()\n",
        "\n",
        "    self.max_disp = max_disp\n",
        "\n",
        "    self.disp = self.max_disp//2\n",
        "\n",
        "    self.spp = SPP()\n",
        "\n",
        "    self.cnn3d_1 = nn.Sequential(nn.Conv3d(in_channels = 64,out_channels = 32, kernel_size = 3, stride = 1, padding = 1, bias = False), nn.BatchNorm3d(32), nn.ReLU(inplace=True),\n",
        "                                 nn.Conv3d(in_channels = 32,out_channels = 32, kernel_size = 3, stride = 1, padding = 1, bias = False), nn.BatchNorm3d(32), nn.ReLU(inplace=True)) # 3DConv_1\n",
        "    \n",
        "    self.cnn3d_2 = nn.Sequential(nn.Conv3d(in_channels = 32,out_channels = 32, kernel_size = 3, stride = 1, padding = 1, bias = False), nn.BatchNorm3d(32), nn.ReLU(inplace=True),\n",
        "                                 nn.Conv3d(in_channels = 32,out_channels = 32, kernel_size = 3, stride = 1, padding = 1, bias = False), nn.BatchNorm3d(32)) # 3DConv_2\n",
        "\n",
        "    self.cnn3d_3 = nn.Sequential(nn.Conv3d(in_channels = 32,out_channels = 32, kernel_size = 3, stride = 1, padding = 1, bias = False), nn.BatchNorm3d(32), nn.ReLU(inplace=True),\n",
        "                                 nn.Conv3d(in_channels = 32,out_channels = 32, kernel_size = 3, stride = 1, padding = 1, bias = False), nn.BatchNorm3d(32)) # 3DConv_3 \n",
        "\n",
        "    self.cnn3d_4 = nn.Sequential(nn.Conv3d(in_channels = 32,out_channels = 32, kernel_size = 3, stride = 1, padding = 1, bias = False), nn.BatchNorm3d(32), nn.ReLU(inplace=True),\n",
        "                                 nn.Conv3d(in_channels = 32,out_channels = 32, kernel_size = 3, stride = 1, padding = 1, bias = False), nn.BatchNorm3d(32)) # 3DConv_4\n",
        "\n",
        "    self.cnn3d_5 = nn.Sequential(nn.Conv3d(in_channels = 32,out_channels = 32, kernel_size = 3, stride = 1, padding = 1, bias = False), nn.BatchNorm3d(32), nn.ReLU(inplace=True),\n",
        "                                 nn.Conv3d(in_channels = 32,out_channels = 32, kernel_size = 3, stride = 1, padding = 1, bias = False), nn.BatchNorm3d(32)) # 3DConv_5\n",
        "    \n",
        "    self.cnn3d_last = nn.Sequential(nn.Conv3d(in_channels = 32,out_channels = 32, kernel_size = 3, stride = 1, padding = 1, bias = False), nn.BatchNorm3d(32), nn.ReLU(inplace=True),\n",
        "                                    nn.Conv3d(in_channels = 32,out_channels = 1, kernel_size = 3, stride = 1, padding = 1, bias = False)) # 3DConv_final\n",
        "\n",
        "    self.disp_reg = disp_reg(self.max_disp)\n",
        "\n",
        "    self.relu1 = nn.ReLU(inplace=True)\n",
        "    self.relu2 = nn.ReLU(inplace=True)\n",
        "    self.relu3 = nn.ReLU(inplace=True)\n",
        "    self.relu4 = nn.ReLU(inplace=True)\n",
        "    \n",
        "\n",
        "  def forward(self,left,right):\n",
        "    left_feat  = self.spp(left) # Extract feature map from left image\n",
        "    right_feat = self.spp(right) # Extract feature map from right image\n",
        "\n",
        "    self.disp_tmp = self.disp // 4\n",
        "    cost = torch.zeros(left_feat.shape[0], left_feat.shape[1] * 2, self.disp_tmp,  left_feat.shape[2], left_feat.shape[3]).float().cuda()\n",
        "\n",
        "    for i in range(0, self.disp_tmp, 1):\n",
        "      if i > 0:\n",
        "        cost[:, left_feat.shape[1]:, i, :, i:]   = left_feat[:,:,:,i:]\n",
        "        cost[:, :left_feat.shape[1], i, :, i:]   = right_feat[:,:,:,:-i]\n",
        "      else:\n",
        "        cost[:, left_feat.shape[1]:, i, :, i:]   = left_feat\n",
        "        cost[:, :left_feat.shape[1], i, :, i:]   = right_feat\n",
        "\n",
        "\n",
        "    cost = self.cnn3d_1(cost)\n",
        "    cost = self.relu1(self.cnn3d_2(cost) + cost) # Additions represent skip connections across 3D Conv layers\n",
        "    cost = self.relu2(self.cnn3d_3(cost) + cost)\n",
        "    cost = self.relu3(self.cnn3d_4(cost) + cost)\n",
        "    cost = self.relu4(self.cnn3d_3(cost) + cost)\n",
        "    cost = self.cnn3d_last(cost)\n",
        "\n",
        "    # Trilinear upsampling to restore image to original size:\n",
        "    cost = F.interpolate(cost, (self.max_disp,left.shape[2],left.shape[3]), mode = 'trilinear') \n",
        "\n",
        "    # Eliminate num_channels dimension(it's 1 anyway):\n",
        "    cost = torch.squeeze(cost,1)\n",
        "    \n",
        "    # Compute softmax across all disparity values:\n",
        "    cost = F.softmax(-cost,dim=1)\n",
        "\n",
        "    #Perform disparity regression as defined in paper:\n",
        "    pred = self.disp_reg(cost)\n",
        "\n",
        "    return pred\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNAVmYw84DTu"
      },
      "source": [
        "**Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJECHlMjHmEh"
      },
      "source": [
        "Batch_size = 3\n",
        "train_set  = StereoDataset('/content/gdrive/My Drive/CATS_txt_rectified/CATS_inferno/', device, dataset_name = 'CATS', mode = 'train',transform=None)\n",
        "valid_set  = StereoDataset('/content/gdrive/My Drive/CATS_txt_rectified/CATS_inferno/', device, dataset_name = 'CATS', mode = 'val',transform=None)\n",
        "\n",
        "# Loader argument dictionary:\n",
        "loader_args = dict(shuffle = True, batch_size = Batch_size, num_workers = 6, pin_memory = True) if torch.cuda.is_available()\\\n",
        "                   else dict(shuffle = True, batch_size = 2)\n",
        "\n",
        "# Creating train and validation loaders:\n",
        "train_loader = DataLoader(train_set,**loader_args)\n",
        "valid_loader = DataLoader(valid_set,**loader_args)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-nCto-9IOiz"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDBjJFXw4J1L"
      },
      "source": [
        "**Training/ Validation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL_MhvocHufn"
      },
      "source": [
        "def validate(val_dataloader, num_batch, model, loss_function):\n",
        "  model.eval()\n",
        "  running_loss, num_sequence = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (left_img, right_img, disp_img) in enumerate(val_dataloader):\n",
        "\n",
        "      # the images go on the gpu\n",
        "      left_img       = left_img.to(device)\n",
        "      right_img      = right_img.to(device)\n",
        "      disp_img_gt    = disp_img.to(device)\n",
        "\n",
        "      disp_img_pred  = model.forward(left_img, right_img)\n",
        "      loss           = loss_function(disp_img_pred, disp_img_gt)\n",
        "\n",
        "      num_sequence, running_loss = num_sequence + 1, running_loss + loss.item()\n",
        "\n",
        "\n",
        "      del left_img,right_img\n",
        "\n",
        "  print('Validation Loss = ' + str(running_loss/len(valid_set)))\n",
        "  return (running_loss/len(valid_set))\n",
        "\n",
        "def train(Batch_size, train_dataloader, val_dataloader):\n",
        "\n",
        "  # Initialize PSMNET object and load it onto GPU:\n",
        "\n",
        "  myModel = PSMNET(max_disp = 192)\n",
        "  # model goes on the gpu\n",
        "  myModel.to(device)\n",
        "  # optimizer, params\n",
        "  num_batch       = Batch_size\n",
        "  num_epochs      = 500\n",
        "  learning_rate   = 1e-3\n",
        "  v_loss = 10000\n",
        "\n",
        "  train_list = []\n",
        "  valid_list = []\n",
        "  epoch_list = []\n",
        "\n",
        "  # initialize the optimizer\n",
        "  optimizer     = torch.optim.Adam(myModel.parameters(), lr=learning_rate)\n",
        "\n",
        "  # initialize the lr scheduler\n",
        "  scheduler     = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=300, verbose = True)\n",
        "\n",
        "  # loss function, l1 loss with smoothness penalty\n",
        "  loss_function = torch.nn.SmoothL1Loss(size_average=True)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss, num_sequence = 0, 0\n",
        "    train_loss = 0\n",
        "    myModel.train(True)\n",
        "\n",
        "    for i, (left_img, right_img, disp_img) in enumerate(train_dataloader):\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # the images go on the gpu\n",
        "      left_img       = left_img.to(device)\n",
        "      right_img      = right_img.to(device)\n",
        "      disp_img_gt    = disp_img.to(device)\n",
        "\n",
        "      disp_img_pred  = myModel.forward(left_img, right_img)\n",
        "      loss = loss_function(disp_img_pred, disp_img_gt)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss = running_loss + loss.item()\n",
        "\n",
        "      del left_img\n",
        "      del right_img\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "    plt.imshow(disp_img_gt.cpu().numpy()[0, :, :])\n",
        "    plt.show()\n",
        "    plt.imshow(disp_img_pred.detach().cpu().numpy()[0, :, :])\n",
        "    plt.show()\n",
        "    print(running_loss/len(train_set))\n",
        "\n",
        "    print('EPOCH = ' + str(epoch))\n",
        "    \n",
        "    print('Training Loss = ' + str(running_loss/len(train_set)))\n",
        "    train_loss = running_loss/len(train_set)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      val_loss = validate(val_dataloader, num_batch, myModel, loss_function)\n",
        "\n",
        "    # Plot training and validation loss:\n",
        "    train_list.append(running_loss/len(train_set))\n",
        "    valid_list.append(val_loss)\n",
        "    epoch_list.append(epoch+1)\n",
        "\n",
        "    plt.plot(epoch_list,train_list)\n",
        "    plt.plot(epoch_list,valid_list)\n",
        "    plt.show()\n",
        "\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Save model\n",
        "    if train_loss < v_loss:\n",
        "      v_loss = train_loss\n",
        "      torch.save(myModel.state_dict(),'pathname')\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4cLGfAG4bKg"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8TcnNTYIZGa"
      },
      "source": [
        "train(Batch_size, train_loader, valid_loader)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXTIECwn4eZv"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PFzbwnfJCR6"
      },
      "source": [
        "%mkdir /content/image_download\n",
        "%mkdir /content/image_download/valid\n",
        "%mkdir /content/image_download/valid_gt\n",
        "%mkdir /content/image_download/train\n",
        "%mkdir /content/image_download/train_gt\n",
        "my_model = PSMNET(max_disp = 192)\n",
        "my_model.to(device)\n",
        "state_dict = (torch.load('pathname'))\n",
        "my_model.load_state_dict(state_dict)\n",
        "\n",
        "my_model.eval()\n",
        "\n",
        "k = 0\n",
        "m = 0\n",
        "f1 = open('cats_final/filenames1/cats_val.txt', 'r')\n",
        "f1 = f1.readlines()\n",
        "f2 = open('cats_final/filenames1/cats_train.txt', 'r')\n",
        "f2 = f2.readlines()\n",
        "with torch.no_grad():\n",
        "  \n",
        "  for i, (left_img, right_img, disp_img) in enumerate(valid_loader):\n",
        "    # the images go on the gpu\n",
        "    left_img       = left_img.to(device)\n",
        "    right_img      = right_img.to(device)\n",
        "    disp_img_gt    = disp_img.to(device)\n",
        "\n",
        "    disp_img_pred  = my_model(left_img, right_img)\n",
        "\n",
        "    # Save predictions in a folder\n",
        "    for j in range(disp_img_pred.shape[0]):\n",
        "      disp_pred = disp_img_pred[j,:,:].detach().cpu().numpy()\n",
        "      disp_pred[disp_pred < 0] = 0\n",
        "      disp_pred[disp_pred > 192.0] = 192.0\n",
        "      plt.imsave('/content/image_download/valid/' + f1[m].split('\\n')[0] ,disp_pred,cmap='gray')\n",
        "      disp_gt = disp_img_gt[j,:,:].cpu().numpy()\n",
        "      plt.imsave('/content/image_download/valid_gt/' + f1[m].split('\\n')[0] ,disp_gt,cmap='gray')\n",
        "      m += 1\n",
        "\n",
        "\n",
        "  for i, (left_img, right_img, disp_img) in enumerate(train_loader):\n",
        "    # the images go on the gpu\n",
        "    left_img       = left_img.to(device)\n",
        "    right_img      = right_img.to(device)\n",
        "    disp_img_gt    = disp_img.to(device)\n",
        "\n",
        "    disp_img_pred  = my_model(left_img, right_img)\n",
        "\n",
        "    \n",
        "    # Save predictions in a folder\n",
        "    for j in range(disp_img_pred.shape[0]):\n",
        "      disp_pred = disp_img_pred[j,:,:].detach().cpu().numpy()\n",
        "      disp_pred[disp_pred < 0] = 0\n",
        "      disp_pred[disp_pred > 192.0] = 192.0\n",
        "      plt.imsave('/content/image_download/train/' + f2[k].split('\\n')[0] ,disp_pred,cmap='gray')\n",
        "      disp_gt = disp_img_gt[j,:,:].cpu().numpy()\n",
        "      plt.imsave('/content/image_download/train_gt/' + f2[k].split('\\n')[0] ,disp_gt,cmap='gray')\n",
        "      # plt.imsave('/content/image_download/train/' + f2[k].split('\\n')[0],disp_img_pred[j,:,:].detach().cpu().numpy(),cmap='gray')\n",
        "      k += 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBRDOfEG4hdx"
      },
      "source": [
        "**Create zip file of evaluated images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt8YbTCK057f"
      },
      "source": [
        "!zip -r inferno_preds.zip image_download/"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4bhHJrU1Dpq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}